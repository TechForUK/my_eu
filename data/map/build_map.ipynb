{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Data for the Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and merge datasets from:\n",
    "    cordis\n",
    "    creative\n",
    "    esif\n",
    "    fts\n",
    "    erasmus\n",
    "    nhs\n",
    "    nweurope\n",
    "    life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from functools import reduce\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Validity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukpostcodes = pd.read_csv('../postcodes/input/ukpostcodes.csv.gz')\n",
    "ukpostcodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_postcodes(df):\n",
    "    assert 'postcode' in df.columns\n",
    "    assert (~df['postcode'].isin(ukpostcodes.postcode)).sum() == 0\n",
    "    \n",
    "def validate_date_range(df):\n",
    "    assert 'start_date' in df.columns\n",
    "    assert 'end_date' in df.columns\n",
    "    assert df['start_date'].dtype == 'datetime64[ns]'\n",
    "    assert df['end_date'].dtype == 'datetime64[ns]'\n",
    "    assert (df['start_date'] > df['end_date']).sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORDIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp7_organizations = pd.read_pickle('../cordis/output/fp7_organizations.pkl.gz')\n",
    "validate_postcodes(fp7_organizations)\n",
    "fp7_organizations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp7_projects = pd.read_pickle('../cordis/output/fp7_projects.pkl.gz')\n",
    "validate_date_range(fp7_projects)\n",
    "fp7_projects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp7 = pd.merge(\n",
    "    fp7_projects, fp7_organizations,\n",
    "    left_on='rcn', right_on='project_rcn', validate='1:m'\n",
    ")\n",
    "fp7['my_eu_id'] = 'fp7_' + fp7.project_rcn.astype('str') + '_' + fp7.organization_id.astype('str')\n",
    "fp7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2020_organizations = pd.read_pickle('../cordis/output/h2020_organizations.pkl.gz')\n",
    "validate_postcodes(h2020_organizations)\n",
    "h2020_organizations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2020_projects = pd.read_pickle('../cordis/output/h2020_projects.pkl.gz')\n",
    "validate_date_range(h2020_projects)\n",
    "h2020_projects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2020 = pd.merge(\n",
    "    h2020_projects, h2020_organizations,\n",
    "    left_on='rcn', right_on='project_rcn', validate='1:m'\n",
    ")\n",
    "h2020['my_eu_id'] = 'h2020_' + h2020.project_rcn.astype('str') + '_' + h2020.organization_id.astype('str')\n",
    "\n",
    "# no briefs available for H2020\n",
    "h2020['related_report_title'] = float('nan')\n",
    "h2020['brief_title'] = float('nan')\n",
    "h2020['teaser'] = float('nan')\n",
    "h2020['article'] = float('nan')\n",
    "h2020['image_path'] = float('nan')\n",
    "\n",
    "h2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(fp7.columns) == set(h2020.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis = pd.concat([fp7, h2020[fp7.columns]])\n",
    "cordis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis['total_cost_gbp'] = (cordis.total_cost_eur * cordis.eur_gbp).round()\n",
    "cordis['max_contribution_gbp'] = (cordis.max_contribution_eur * cordis.eur_gbp).round()\n",
    "cordis['contribution_gbp'] = (cordis.contribution_eur * cordis.eur_gbp).round()\n",
    "cordis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cordis.contribution_eur > cordis.total_cost_eur + 0.1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    cordis.start_date.isna().sum(),\n",
    "    cordis.end_date.isna().sum()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creative Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_organisations = pd.read_pickle('../creative/output/creative_europe_organisations.pkl.gz')\n",
    "creative_organisations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_projects = pd.read_pickle('../creative/output/creative_europe_projects.pkl.gz')\n",
    "creative_projects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative = pd.merge(creative_projects, creative_organisations, on='project_number', validate='1:m')\n",
    "creative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_postcodes(creative)\n",
    "validate_date_range(creative)\n",
    "creative['max_contribution_gbp'] = (creative.max_contribution_eur * creative.eur_gbp).round()\n",
    "creative['my_eu_id'] = \\\n",
    "    'creative_' + creative.project_number + '_' + \\\n",
    "    creative.partner_number.apply('{:.0f}'.format).\\\n",
    "    str.replace('nan', 'coordinator', regex=False)\n",
    "assert creative.shape[0] == creative.my_eu_id.unique().shape[0]\n",
    "creative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative.results_available.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative.results_url[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[creative.start_date.isna().sum(), creative.end_date.isna().sum()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESIF (ESF/ERDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_england = pd.read_pickle('../esif/output/esif_england_2014_2020.pkl.gz')\n",
    "validate_postcodes(esif_england)\n",
    "validate_date_range(esif_england)\n",
    "esif_england.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_ni = pd.read_pickle('../esif/output/esif_ni_2014_2020.pkl.gz')\n",
    "validate_postcodes(esif_ni)\n",
    "validate_date_range(esif_ni)\n",
    "esif_ni.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_scotland = pd.read_pickle('../esif/output/esif_scotland.pkl.gz')\n",
    "validate_postcodes(esif_scotland)\n",
    "validate_date_range(esif_scotland)\n",
    "esif_scotland.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esif_wales = pd.read_pickle('../esif/output/esif_wales.pkl.gz')\n",
    "validate_postcodes(esif_wales)\n",
    "validate_date_range(esif_wales)\n",
    "esif_wales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(esif_england.columns) == set(esif_ni.columns)\n",
    "assert set(esif_england.columns) == set(esif_scotland.columns)\n",
    "assert set(esif_england.columns) == set(esif_wales.columns)\n",
    "esif_columns = esif_england.columns\n",
    "esif = pd.concat([\n",
    "    esif_england,\n",
    "    esif_ni[esif_columns],\n",
    "    esif_scotland[esif_columns],\n",
    "    esif_wales[esif_columns]\n",
    "])\n",
    "esif.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[esif.start_date.isna().sum(), esif.end_date.isna().sum()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts_2016 = pd.read_pickle('../fts/output/fts_2016.pkl.gz')\n",
    "validate_postcodes(fts_2016)\n",
    "fts_2016['year'] = 2016\n",
    "fts_2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts_2017 = pd.read_pickle('../fts/output/fts_2017.pkl.gz')\n",
    "validate_postcodes(fts_2017)\n",
    "fts_2017['year'] = 2017\n",
    "fts_2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts = pd.concat([fts_2016, fts_2017])\n",
    "fts['amount_gbp'] = (fts.amount * fts.eur_gbp).round()\n",
    "fts['total_amount_gbp'] = (fts.total_amount_eur * fts.eur_gbp).round()\n",
    "fts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erasmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erasmus_organisations = pd.read_pickle('../erasmus/output/erasmus_mobility_organisations.pkl.gz')\n",
    "erasmus_organisations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erasmus_projects = pd.read_pickle('../erasmus/output/erasmus_mobility_projects.pkl.gz')\n",
    "erasmus_projects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erasmus = pd.merge(erasmus_projects, erasmus_organisations, on='project_identifier', validate='1:m')\n",
    "erasmus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_postcodes(erasmus)\n",
    "\n",
    "erasmus['max_contribution_gbp'] = (erasmus.max_contribution_eur * erasmus.eur_gbp).round()\n",
    "erasmus['my_eu_id'] = \\\n",
    "    'erasmus_' + erasmus.project_identifier + '_' + \\\n",
    "    erasmus.partner_number.apply('{:.0f}'.format).\\\n",
    "    str.replace('nan', 'coordinator', regex=False)\n",
    "assert erasmus.shape[0] == erasmus.my_eu_id.unique().shape[0]\n",
    "erasmus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhs_staff = pd.read_pickle('../nhs/output/staff.pkl.gz')\n",
    "nhs_hospital_postcodes = pd.read_pickle('../nhs/output/hospital_postcodes.pkl.gz')\n",
    "validate_postcodes(nhs_hospital_postcodes)\n",
    "[nhs_staff.shape, nhs_hospital_postcodes.shape, nhs_hospital_postcodes.hospital_organisation.nunique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy amount so we can put it in\n",
    "nhs_hospital_postcodes['zero'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhs_hospital_postcodes['my_eu_id'] = \\\n",
    "    'nhs_' + nhs_hospital_postcodes.hospital_organisation\n",
    "nhs_hospital_postcodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interreg NW Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nweurope = pd.read_pickle('../nweurope/output/interreg_beneficiaries.pkl.gz')\n",
    "validate_postcodes(nweurope)\n",
    "nweurope.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life = pd.read_pickle('../life/output/life.pkl.gz')\n",
    "validate_postcodes(life)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life['total_budget_gbp'] = (life.total_budget_eur * life.eur_gbp).round()\n",
    "life['eu_contribution_gbp'] = (life.eu_contribution_eur * life.eur_gbp).round()\n",
    "life.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 1: All Points on Map, Data by District\n",
    "\n",
    "This should make the map look fairly similar to how it looks now, so it seems like a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_PLACES = [\n",
    "    (cordis, 'contribution_gbp', 'money'),\n",
    "    (creative, 'max_contribution_gbp', 'money'),\n",
    "    (esif, 'eu_investment', 'money'),\n",
    "    (fts.drop('amount', axis=1), 'amount_gbp', 'money'),\n",
    "    (erasmus, 'max_contribution_gbp', 'money'),\n",
    "    (nhs_hospital_postcodes, 'zero', 'hospital'),\n",
    "    (nweurope, 'funding', 'money'),\n",
    "    (life, 'eu_contribution_gbp', 'money')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GeoJSON is very inefficient for representing a bunch of points, so let's use a relatively simple packed format.\n",
    "```\n",
    "min_longitude min_latitude\n",
    "outcode incode delta_longitude delta_latitude incode delta_longitude delta_latitude\n",
    "```\n",
    "We need [about 4 decimal places](https://gis.stackexchange.com/questions/8650/measuring-accuracy-of-latitude-and-longitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_outward_and_inward_codes(df):\n",
    "    df['outward_code'] = df.postcode.str.split(' ').str[0]\n",
    "    df['inward_code'] = df.postcode.str.split(' ').str[1]\n",
    "    return df\n",
    "\n",
    "def add_packed_icon_mask(df):\n",
    "    def pack_icon_mask(icons):\n",
    "        # So far we just have one bit in our mask; we may have more.\n",
    "        if 'hospital' in icons:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    MASK_BITS = 1\n",
    "    assert df.amount.max() < 2**(32 - MASK_BITS)\n",
    "    df['icon_mask'] = df.icons.apply(pack_icon_mask).astype('uint32')\n",
    "    df['packed_amount'] = np.bitwise_or(\n",
    "        np.left_shift(df.amount, MASK_BITS), df.icon_mask\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def pack_geocoded_postcodes(dfs):\n",
    "    all_postcode_amounts = pd.concat([\n",
    "        df.rename(columns={amount_column: 'amount'}).assign(icons=icon)\\\n",
    "        [['postcode', 'amount', 'icons']]\n",
    "        for df, amount_column, icon in dfs\n",
    "    ])\n",
    "    postcode_amounts = all_postcode_amounts.groupby('postcode').\\\n",
    "        aggregate({'amount': sum, 'icons': lambda icons: set(icons)})\n",
    "    postcode_amounts.reset_index(inplace=True)\n",
    "    postcode_amounts.amount = postcode_amounts.amount.astype('uint32')\n",
    "    add_outward_and_inward_codes(postcode_amounts)\n",
    "    add_packed_icon_mask(postcode_amounts)\n",
    "    \n",
    "    geocoded_postcodes = pd.merge(postcode_amounts, ukpostcodes, validate='1:1')\n",
    "    \n",
    "    min_longitude = geocoded_postcodes.longitude.min()\n",
    "    min_latitude = geocoded_postcodes.latitude.min()\n",
    "    \n",
    "    geocoded_postcodes['delta_longitude'] = geocoded_postcodes.longitude - min_longitude\n",
    "    geocoded_postcodes['delta_latitude'] = geocoded_postcodes.latitude - min_latitude\n",
    "    \n",
    "    return {\n",
    "        'min_longitude': min_longitude,\n",
    "        'min_latitude': min_latitude,\n",
    "        'geocoded_postcodes': geocoded_postcodes\n",
    "    }\n",
    "\n",
    "packed_postcodes = pack_geocoded_postcodes(ALL_PLACES)\n",
    "[\n",
    "    packed_postcodes['min_longitude'],\n",
    "    packed_postcodes['min_latitude'],\n",
    "    packed_postcodes['geocoded_postcodes'].shape[0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_postcodes['geocoded_postcodes'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_packed_postcode_json(packed_postcodes):\n",
    "    packed_postcodes = packed_postcodes.copy()\n",
    "   \n",
    "    grouped_postcodes = packed_postcodes['geocoded_postcodes'].\\\n",
    "        sort_values('outward_code').groupby('outward_code')\n",
    "     \n",
    "    def make_code_tuples(row):\n",
    "        coordinate = '{0:.4f}'\n",
    "        return [\n",
    "            row['inward_code'],\n",
    "            float(coordinate.format(row['delta_longitude'])),\n",
    "            float(coordinate.format(row['delta_latitude'])),\n",
    "            row['packed_amount']\n",
    "        ]\n",
    "    \n",
    "    postcodes = {}\n",
    "    for outward_code, group in grouped_postcodes:\n",
    "        postcodes[outward_code] = [\n",
    "            x\n",
    "            for code in group.sort_values('inward_code').apply(make_code_tuples, axis=1)\n",
    "            for x in code\n",
    "        ]\n",
    "\n",
    "    min_coordinate = '{0:.6f}'\n",
    "    return {\n",
    "        'min_longitude': float(min_coordinate.format(packed_postcodes['min_longitude'])),\n",
    "        'min_latitude': float(min_coordinate.format(packed_postcodes['min_latitude'])),\n",
    "        'postcodes': postcodes\n",
    "    }\n",
    "\n",
    "with open('output/packed_postcodes.data.json', 'w') as file:\n",
    "    json.dump(make_packed_postcode_json(packed_postcodes), file, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[place[0].shape[0] for place in ALL_PLACES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(place[0].shape[0] for place in ALL_PLACES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data by District\n",
    "\n",
    "#### CORDIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PROJECTS = 50\n",
    "\n",
    "# Dump to JSON using pandas, because it puts in nulls instead of NaNs for\n",
    "# missing values. Then load the JSON into dicts for \n",
    "def make_district_data_json(df, name):\n",
    "    key = ['outwardCode', 'inwardCode']\n",
    "    def to_json(group):\n",
    "        group.drop(key, axis=1, inplace=True)\n",
    "        return json.loads(group.to_json(orient='split', index=False))\n",
    "    grouped = df.groupby(key).apply(to_json)\n",
    "    \n",
    "    grouped = grouped.reset_index()\n",
    "    grouped.columns = key + [name]\n",
    "    \n",
    "    for _key, row in grouped.iterrows():\n",
    "        count = len(row[name]['data'])\n",
    "        if count > MAX_PROJECTS:\n",
    "            row[name]['data'] = row[name]['data'][0:MAX_PROJECTS]\n",
    "            row[name]['extra'] = count - MAX_PROJECTS\n",
    "        \n",
    "    return grouped\n",
    "\n",
    "def make_cordis_district_data(cordis):\n",
    "    cordis = add_outward_and_inward_codes(cordis.copy())\n",
    "\n",
    "    cordis = cordis[[\n",
    "        'outward_code',\n",
    "        'inward_code',\n",
    "        'start_date', 'end_date',\n",
    "        'title',\n",
    "        'name', # of organization\n",
    "        'objective',\n",
    "        'contribution_gbp',\n",
    "        'total_cost_gbp',\n",
    "        'num_countries',\n",
    "        'num_organizations',\n",
    "        'acronym',\n",
    "        'project_url',\n",
    "        'organization_url',\n",
    "        'image_path',\n",
    "        'my_eu_id'\n",
    "    ]]\n",
    "    \n",
    "    cordis.rename({\n",
    "        'outward_code': 'outwardCode',\n",
    "        'inward_code': 'inwardCode',\n",
    "        'start_date': 'startDate',\n",
    "        'end_date': 'endDate',\n",
    "        'title': 'projectTitle',\n",
    "        'name': 'organisationName',\n",
    "        'contribution_gbp': 'contribution',\n",
    "        'total_cost_gbp': 'totalCost',\n",
    "        'num_countries': 'numCountries',\n",
    "        'num_organizations': 'numOrganisations',\n",
    "        'project_url': 'projectUrl',\n",
    "        'organization_url': 'organisationUrl',\n",
    "        'image_path': 'imagePath',\n",
    "        'my_eu_id': 'myEuId'\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    cordis.sort_values(\n",
    "        ['outwardCode', 'inwardCode', 'contribution'],\n",
    "        ascending=[True, True, False],\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    return make_district_data_json(cordis, 'cordis')\n",
    "\n",
    "cordis_district_data = make_cordis_district_data(cordis)\n",
    "cordis_district_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creative Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_creative_district_data(creative):\n",
    "    creative = add_outward_and_inward_codes(creative.copy())\n",
    "    \n",
    "    coordinators = creative[creative.organisation_coordinator]\n",
    "    coordinators = coordinators[['project_number', 'organisation_name']]\n",
    "    creative = pd.merge(\n",
    "        creative, coordinators,\n",
    "        how='left', on='project_number', suffixes=('', '_coordinator'))\n",
    "\n",
    "    creative = creative[[\n",
    "        'outward_code',\n",
    "        'inward_code',\n",
    "        'start_date', 'end_date',\n",
    "        'project',\n",
    "        'organisation_name',\n",
    "        'max_contribution_gbp',\n",
    "        'num_countries',\n",
    "        'num_organisations',\n",
    "        'summary',\n",
    "        'organisation_website',\n",
    "        'organisation_name_coordinator',\n",
    "        'my_eu_id'\n",
    "    ]]\n",
    "    \n",
    "    creative.rename({\n",
    "        'outward_code': 'outwardCode',\n",
    "        'inward_code': 'inwardCode',\n",
    "        'start_date': 'startDate',\n",
    "        'end_date': 'endDate',\n",
    "        'organisation_name': 'organisationName',\n",
    "        'max_contribution_gbp': 'maxContribution',\n",
    "        'num_countries': 'numCountries',\n",
    "        'num_organisations': 'numOrganisations',\n",
    "        'organisation_website': 'organisationWebsite',\n",
    "        'organisation_name_coordinator': 'coordinatorName',\n",
    "        'my_eu_id': 'myEuId'\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    creative.sort_values(\n",
    "        ['outwardCode', 'inwardCode', 'maxContribution'],\n",
    "        ascending=[True, True, False],\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    return make_district_data_json(creative, 'creative')\n",
    "\n",
    "creative_district_data = make_creative_district_data(creative)\n",
    "creative_district_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_esif_district_data(esif):\n",
    "    esif = add_outward_and_inward_codes(esif.copy())\n",
    "    esif = esif[[\n",
    "        'outward_code',\n",
    "        'inward_code',\n",
    "        'start_date', 'end_date',\n",
    "        'project',\n",
    "        'beneficiary',\n",
    "        'summary',\n",
    "        'funds',\n",
    "        'eu_investment',\n",
    "        'project_cost',\n",
    "        'my_eu_id'\n",
    "    ]]\n",
    "    \n",
    "    esif.rename({\n",
    "        'outward_code': 'outwardCode',\n",
    "        'inward_code': 'inwardCode',\n",
    "        'start_date': 'startDate',\n",
    "        'end_date': 'endDate',\n",
    "        'project': 'projectTitle',\n",
    "        'beneficiary': 'organisationName',\n",
    "        'eu_investment': 'euInvestment',\n",
    "        'project_cost': 'projectCost',\n",
    "        'my_eu_id': 'myEuId'\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    esif.sort_values(\n",
    "        ['outwardCode', 'inwardCode', 'euInvestment'],\n",
    "        ascending=[True, True, False],\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    return make_district_data_json(esif, 'esif')\n",
    "\n",
    "esif_district_data = make_esif_district_data(esif)\n",
    "esif_district_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fts_district_data(fts):\n",
    "    fts = add_outward_and_inward_codes(fts.copy())\n",
    "    fts = fts[[\n",
    "        'outward_code',\n",
    "        'inward_code',\n",
    "        'year',\n",
    "        'beneficiary',\n",
    "        'amount_gbp',\n",
    "        'budget_line_name_and_number',\n",
    "        'my_eu_id'\n",
    "    ]]\n",
    "    \n",
    "    fts.rename({\n",
    "        'outward_code': 'outwardCode',\n",
    "        'inward_code': 'inwardCode',\n",
    "        'amount_gbp': 'amount',\n",
    "        'budget_line_name_and_number': 'budgetLineNameAndNumber',\n",
    "        'my_eu_id': 'myEuId'\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    fts.sort_values(\n",
    "        ['outwardCode', 'inwardCode', 'amount'],\n",
    "        ascending=[True, True, False],\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    return make_district_data_json(fts, 'fts')\n",
    "\n",
    "fts_district_data = make_fts_district_data(fts)\n",
    "fts_district_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erasmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_erasmus_district_data(erasmus):\n",
    "    erasmus = add_outward_and_inward_codes(erasmus.copy())\n",
    "\n",
    "    coordinators = erasmus[erasmus.organisation_coordinator]\n",
    "    coordinators = coordinators[['project_identifier', 'organisation_name']]\n",
    "    erasmus = pd.merge(\n",
    "        erasmus, coordinators,\n",
    "        how='left', on='project_identifier', suffixes=('', '_coordinator'))\n",
    "\n",
    "    erasmus = erasmus[[\n",
    "        'outward_code',\n",
    "        'inward_code',\n",
    "        'call_year',\n",
    "        'project',\n",
    "        'organisation_name',\n",
    "        'max_contribution_gbp',\n",
    "        'num_countries',\n",
    "        'num_organisations',\n",
    "        'summary',\n",
    "        'organisation_website',\n",
    "        'organisation_name_coordinator',\n",
    "        'my_eu_id'\n",
    "    ]]\n",
    "    \n",
    "    erasmus.rename({\n",
    "        'outward_code': 'outwardCode',\n",
    "        'inward_code': 'inwardCode',\n",
    "        'call_year': 'callYear',\n",
    "        'organisation_name': 'organisationName',\n",
    "        'max_contribution_gbp': 'maxContribution',\n",
    "        'num_countries': 'numCountries',\n",
    "        'num_organisations': 'numOrganisations',\n",
    "        'organisation_website': 'organisationWebsite',\n",
    "        'organisation_name_coordinator': 'coordinatorName',\n",
    "        'my_eu_id': 'myEuId'\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    erasmus.sort_values(\n",
    "        ['outwardCode', 'inwardCode', 'maxContribution'],\n",
    "        ascending=[True, True, False],\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    return make_district_data_json(erasmus, 'erasmus')\n",
    "\n",
    "erasmus_district_data = make_erasmus_district_data(erasmus)\n",
    "erasmus_district_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NHS\n",
    "\n",
    "Just store the organisation key for now. The data doesn't really fit with the district model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nhs_district_data(nhs_hospital_postcodes, nhs_staff):\n",
    "    nhs = add_outward_and_inward_codes(pd.merge(\n",
    "        nhs_hospital_postcodes, nhs_staff, on='organisation', validate='m:1'\n",
    "    ))\n",
    "    \n",
    "    nhs.sort_values(\n",
    "        ['outward_code', 'inward_code', 'eu_nurses'],\n",
    "        ascending=[True, True, False],\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    nhs = nhs[[\n",
    "        'outward_code',\n",
    "        'inward_code',\n",
    "        'organisation',\n",
    "        'hospital_name',\n",
    "        'my_eu_id'\n",
    "    ]]\n",
    "    \n",
    "    nhs.rename({\n",
    "        'outward_code': 'outwardCode',\n",
    "        'inward_code': 'inwardCode',\n",
    "        'hospital_name': 'hospitalName',\n",
    "        'my_eu_id': 'myEuId'\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    return make_district_data_json(nhs, 'nhs')\n",
    "\n",
    "nhs_district_data = make_nhs_district_data(nhs_hospital_postcodes, nhs_staff)\n",
    "nhs_district_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interreg NW Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nweurope.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nweurope.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nweurope_district_data(nweurope):\n",
    "    nweurope = add_outward_and_inward_codes(nweurope.copy())\n",
    "    nweurope = nweurope[[\n",
    "        'outward_code',\n",
    "        'inward_code',\n",
    "        'beneficiary',\n",
    "        'project',\n",
    "        'start_date',\n",
    "        'end_date',\n",
    "        'funding',\n",
    "        'union_cofinancing',\n",
    "        'nwreg_benefs_id'\n",
    "    ]]\n",
    "    \n",
    "    nweurope.rename({\n",
    "        'outward_code': 'outwardCode',\n",
    "        'inward_code': 'inwardCode',\n",
    "        'start_date': 'startDate',\n",
    "        'end_date': 'endDate',\n",
    "        'union_cofinancing': 'unionCofinancing',\n",
    "        'nwreg_benefs_id': 'myEuId'\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    nweurope.sort_values(\n",
    "        ['outwardCode', 'inwardCode', 'unionCofinancing'],\n",
    "        ascending=[True, True, False],\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    return make_district_data_json(nweurope, 'nweurope')\n",
    "\n",
    "nweurope_district_data = make_nweurope_district_data(nweurope)\n",
    "nweurope_district_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_life_district_data(life):\n",
    "    life = add_outward_and_inward_codes(life.copy())\n",
    "    life = life[[\n",
    "        'outward_code',\n",
    "        'inward_code',\n",
    "        'year',\n",
    "        'project_title',\n",
    "        'coordinator',\n",
    "        'total_budget_gbp',\n",
    "        'eu_contribution_gbp',\n",
    "        'background',\n",
    "        'project_url',\n",
    "        'website',\n",
    "        'my_eu_id'\n",
    "    ]]\n",
    "    \n",
    "    life.rename({\n",
    "        'outward_code': 'outwardCode',\n",
    "        'inward_code': 'inwardCode',\n",
    "        'total_budget_gbp': 'amount',\n",
    "        'eu_contribution_gbp': 'euContribution',\n",
    "        'my_eu_id': 'myEuId'\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    life.sort_values(\n",
    "        ['outwardCode', 'inwardCode', 'amount'],\n",
    "        ascending=[True, True, False],\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    return make_district_data_json(life, 'life')\n",
    "\n",
    "life_district_data = make_life_district_data(life)\n",
    "life_district_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data\n",
    "\n",
    "```\n",
    "districtData.postcodes['A11'].cordis.data\n",
    "districtData.postcodes['A11'].cordis.extra\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_DISTRICT_DATA = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=('outwardCode', 'inwardCode'), how='outer'), [\n",
    "    cordis_district_data,\n",
    "    creative_district_data,\n",
    "    esif_district_data,\n",
    "    fts_district_data,\n",
    "    erasmus_district_data,\n",
    "    nhs_district_data,\n",
    "    nweurope_district_data,\n",
    "    life_district_data    \n",
    "])\n",
    "ALL_DATASET_NAMES = list(set(ALL_DISTRICT_DATA.columns) - set(['outwardCode', 'inwardCode']))\n",
    "    \n",
    "def save_columns():\n",
    "    def find_columns(data):\n",
    "        return data[~data.isna()].iloc[0]['columns']\n",
    "  \n",
    "    columns = {\n",
    "        dataset_name: find_columns(ALL_DISTRICT_DATA[dataset_name])\n",
    "        for dataset_name in ALL_DATASET_NAMES\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join('output', 'district_columns.json'), 'w') as file:\n",
    "        json.dump(columns, file, sort_keys=True)\n",
    "save_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_DISTRICT_DATA[(~ALL_DISTRICT_DATA[ALL_DATASET_NAMES].isna()).sum(axis=1) > 4][['outwardCode', 'inwardCode']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_district_data(all_data):\n",
    "    def find_dataset_data(row):\n",
    "        dataset_data = row.drop(['outwardCode', 'inwardCode']).dropna().to_dict()\n",
    "        return {\n",
    "            dataset: { k: v for k, v in json.items() if k != 'columns' }\n",
    "            for dataset, json in dataset_data.items()\n",
    "        }\n",
    "    \n",
    "    def make_merged_district_data(outward_code, group):\n",
    "        return {\n",
    "            'outwardCode': outward_code,\n",
    "            'postcodes': {\n",
    "                row.inwardCode: find_dataset_data(row)\n",
    "                for _index, row in group.iterrows()\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        outward_code: make_merged_district_data(outward_code, group)\n",
    "        for outward_code, group in all_data.groupby('outwardCode')\n",
    "    }\n",
    "\n",
    "district_data = merge_district_data(ALL_DISTRICT_DATA)\n",
    "district_data['CA4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DISTRICT_PATH = 'output/district'\n",
    "\n",
    "def list_district_data(path):\n",
    "    return glob.glob(os.path.join(path, '*.data.json'))\n",
    "\n",
    "def clear_district_data(path):\n",
    "    for f in list_district_data(path):\n",
    "        os.remove(f)\n",
    "\n",
    "def write_district_data(district_data, path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    clear_district_data(path)\n",
    "    for outward_code, data in district_data.items():\n",
    "        output_pathname = os.path.join(path, outward_code + '.data.json')\n",
    "        with open(output_pathname, 'w') as file:\n",
    "            json.dump(data, file, sort_keys=True)\n",
    "write_district_data(district_data, OUTPUT_DISTRICT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_district_data_stats():\n",
    "    files = list_district_data(OUTPUT_DISTRICT_PATH)\n",
    "    return pd.DataFrame({\n",
    "        'file': [file for file in files],\n",
    "        'byte_size': [os.stat(file).st_size for file in files]\n",
    "    })\n",
    "district_data_stats = find_district_data_stats()\n",
    "district_data_stats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_data_stats.byte_size.sum() / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_data_stats[district_data_stats.byte_size > 1024*1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_data_stats.describe().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Index\n",
    "\n",
    "Generate a JS file that webpack can use to make paths for all of the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_district_data_js():\n",
    "    data_files = list_district_data(OUTPUT_DISTRICT_PATH)\n",
    "    \n",
    "    def make_require(data_file):\n",
    "        basename = os.path.basename(data_file)\n",
    "        pathname = os.path.join('.', 'district', basename)\n",
    "        outward_code = basename.split('.')[0]\n",
    "        return \"  {}: require('{}')\".format(outward_code, pathname)\n",
    "\n",
    "    with open('output/district.js', 'w') as file:\n",
    "        file.write('// NB: This file is generated automatically. Do not edit.\\n')\n",
    "        file.write('export default {\\n')\n",
    "        requires = [\n",
    "            make_require(data_file)\n",
    "            for data_file in sorted(data_files)\n",
    "        ]\n",
    "        file.write(',\\n'.join(requires))\n",
    "        file.write('\\n}\\n')\n",
    "write_district_data_js()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NHS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[nhs_staff.shape[0], nhs_hospital_postcodes.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhs_hospital_postcodes.groupby('hospital_organisation').organisation.count().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_nhs_staff_data():\n",
    "    with open('output/nhs_staff.json', 'w') as file:\n",
    "        file.write(\n",
    "            nhs_staff.sort_values('organisation', ascending=True).\\\n",
    "            to_json(orient='split', index=False))\n",
    "write_nhs_staff_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
